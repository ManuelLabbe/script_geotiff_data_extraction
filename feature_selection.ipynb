{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "pirange_1 = pd.read_csv('PIRange_1')\n",
    "rosetta_mean_1 = pd.read_csv('rosetta_mean_1')\n",
    "soilmaps_mean_1 = pd.read_csv('soilmaps_mean_1')\n",
    "textural_classes_1 = pd.read_csv('textural_classes_1')\n",
    "db = pd.read_csv('New_DB_2.csv')\n",
    "\n",
    "textural_classes_1 = textural_classes_1.drop(columns=['Unnamed: 0.1', 'Unnamed: 0'])\n",
    "rosetta_mean_1 = rosetta_mean_1.drop(columns=['Unnamed: 0.1', 'Unnamed: 0'])\n",
    "soilmaps_mean_1 = soilmaps_mean_1.drop(columns=['Unnamed: 0.1', 'Unnamed: 0'])\n",
    "pirange_1 = pirange_1.drop(columns=['Unnamed: 0.1', 'Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_latlon = ['Latitud', 'Longitud']\n",
    "rosetta_mean_1 = rosetta_mean_1.drop(columns=cols_latlon)\n",
    "pirange_1 = pirange_1.drop(columns=cols_latlon)\n",
    "textural_classes_1 = textural_classes_1.drop(columns=cols_latlon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0 = pd.read_csv('tif_data_0_01')\n",
    "data_0['Valor'] = 0\n",
    "data_1 = pd.concat([soilmaps_mean_1, rosetta_mean_1, pirange_1, textural_classes_1], axis=1)\n",
    "data_1['Valor'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0 = pd.read_csv('tif_data_0_01')\n",
    "data_0 = data_0.drop(columns=['Unnamed: 0'])\n",
    "data_0['Valor'] = 0\n",
    "data_1 = pd.concat([soilmaps_mean_1, rosetta_mean_1, pirange_1, textural_classes_1], axis=1)\n",
    "data_1['Valor'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Luego de drop duplicates: (1247, 143)\n",
      "\n",
      "Luego de drop duplicates: (602, 143)\n"
     ]
    }
   ],
   "source": [
    "print(f'Luego de drop duplicates: {data_0.shape}\\n')\n",
    "data_0 = data_0.drop_duplicates(subset=['Latitud', 'Longitud', 'Fecha Evento'])\n",
    "print(f'Luego de drop duplicates: {data_0.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Luego de drop duplicates: (458, 106)\n",
      "\n",
      "Luego de drop duplicates: (420, 106)\n"
     ]
    }
   ],
   "source": [
    "print(f'Luego de drop duplicates: {data_1.shape}\\n')\n",
    "data_1 = data_1.drop_duplicates()\n",
    "print(f'Luego de drop duplicates: {data_1.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(533, 17)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_1 = db[db['Valor'] == 1]\n",
    "db_0 = db[db['Valor'] == 0]\n",
    "db_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0 = pd.merge(data_0, db_0, on=['Latitud', 'Longitud', 'Valor', 'Fecha Evento'], how='inner')\n",
    "data_1 = pd.merge(data_1, db_1, on=['Latitud', 'Longitud', 'Valor'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(df):\n",
    "    # Crear una lista con los nombres de las columnas renombradas\n",
    "    new_columns = []\n",
    "    for col in df.columns:\n",
    "        if col.endswith('_x') or col.endswith('_y'):\n",
    "            new_columns.append(col[:-2])\n",
    "        else:\n",
    "            new_columns.append(col)\n",
    "    \n",
    "    # Renombrar las columnas del DataFrame\n",
    "    df.columns = new_columns\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(531, 156)\n"
     ]
    }
   ],
   "source": [
    "data_0 = rename_columns(data_0)\n",
    "data_1 = rename_columns(data_1)\n",
    "print(data_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(531, 156) (458, 120)\n",
      "Total data: 989\n"
     ]
    }
   ],
   "source": [
    "print(data_0.shape, data_1.shape)\n",
    "print(f'Total data: {data_0.shape[0] + data_1.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDuplicateColumns(df):\n",
    " \n",
    "    # Create an empty set\n",
    "    duplicateColumnNames = set()\n",
    " \n",
    "    # Iterate through all the columns of dataframe\n",
    "    for x in range(df.shape[1]):\n",
    " \n",
    "        # Take column at xth index.\n",
    "        col = df.iloc[:, x]\n",
    " \n",
    "        # Iterate through all the columns\n",
    "        for y in range(x + 1, df.shape[1]):\n",
    " \n",
    "            # Take column at yth index.\n",
    "            otherCol = df.iloc[:, y]\n",
    " \n",
    "            # Check if two columns at x & y\n",
    "            if col.equals(otherCol):\n",
    "                duplicateColumnNames.add(df.columns.values[y])\n",
    " \n",
    "    return list(duplicateColumnNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import re\\ndata_0_aux = data_0_aux.rename(columns= lambda x: re.sub(r'\\\\.', ' ', x))\\ndata_1_aux = data_1_aux.rename(columns= lambda x: re.sub(r'\\\\.', ' ', x))\""
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import re\n",
    "data_0_aux = data_0_aux.rename(columns= lambda x: re.sub(r'\\.', ' ', x))\n",
    "data_1_aux = data_1_aux.rename(columns= lambda x: re.sub(r'\\.', ' ', x))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas en común: ['theta_r_5-15cm.tif', 'Bulkd.0-5cm.tif', 'Bulkd.60-100cm.tif', 'Clay.15-30cm.tif', 'PIRange_Sand.100-200cm.tif', 'PIRange_Bulkd.0-5cm.tif', 'ksat_100-200cm.tif', 'theta_s_60-100cm.tif', 'Sand.100-200cm.tif', 'Silt.30-60cm.tif', 'PIRange_Bulkd.5-15cm.tif', 'FC.15-30cm.tif', 'PWP.100-200cm.tif', 'theta_r_0-5cm.tif', 'theta_r_100-200cm.tif', 'PWP.5-15cm.tif', 'theta_s_100-200cm.tif', 'AvMoist.100-200cm.tif', 'Silt.60-100cm.tif', 'Sand.0-5cm.tif', 'alpha_30-60cm.tif', 'Bulkd.30-60cm.tif', 'n_0-5cm.tif', 'alpha_60-100cm.tif', 'PIRange_Sand.5-15cm.tif', 'AvMoist.30-60cm.tif', 'Silt.0-5cm.tif', 'theta_s_30-60cm.tif', 'Silt.15-30cm.tif', 'AWC_15-30cm.tif', 'n_5-15cm.tif', 'theta_r_15-30cm.tif', 'Clay.30-60cm.tif', 'PIRange_Clay.15-30cm.tif', 'AWC_5-15cm.tif', 'Valor', 'AWC_30-60cm.tif', 'Clay.5-15cm.tif', 'Tex_Class.15-30cm.tif', 'Fecha Evento', 'PIRange_Bulkd.30-60cm.tif', 'theta_r_60-100cm.tif', 'PWP.60-100cm.tif', 'Sistema Georeferencia', 'Cota (m.s.n.m)', 'FC.30-60cm.tif', 'theta_s_5-15cm.tif', 'Tex_Class.30-60cm.tif', 'PIRange_Clay.60-100cm.tif', 'alpha_100-200cm.tif', 'Clay.0-5cm.tif', 'PIRange_Clay.0-5cm.tif', 'Tex_Class.100-200cm.tif', 'Bulkd.15-30cm.tif', 'Bulkd.5-15cm.tif', 'theta_s_0-5cm.tif', 'FC.100-200cm.tif', 'alpha_15-30cm.tif', 'PIRange_Sand.0-5cm.tif', 'valor_humedad_suelo4', 'Sand.60-100cm.tif', 'n_100-200cm.tif', 'Sand.15-30cm.tif', 'Tex_Class.60-100cm.tif', 'ksat_0-5cm.tif', 'AvMoist.0-5cm.tif', 'valor_humedad_suelo2', 'alpha_5-15cm.tif', 'AvMoist.5-15cm.tif', 'alpha_0-5cm.tif', 'FC.5-15cm.tif', 'Tex_Class.5-15cm.tif', 'Sand.30-60cm.tif', 'PWP.15-30cm.tif', 'Silt.5-15cm.tif', 'PIRange_Bulkd.60-100cm.tif', 'PIRange_Clay.30-60cm.tif', 'Región', 'PWP.0-5cm.tif', 'PIRange_Sand.60-100cm.tif', 'PIRange_Sand.15-30cm.tif', 'Sand.5-15cm.tif', 'AWC_60-100cm.tif', 'valor_humedad_suelo3', 'slope', 'Comuna', 'Factor desencadenante', 'PIRange_Clay.100-200cm.tif', 'theta_r_30-60cm.tif', 'Tipo Remoción en masa', 'Bulkd.100-200cm.tif', 'theta_s_15-30cm.tif', 'PIRange_Bulkd.15-30cm.tif', 'Clay.60-100cm.tif', 'Silt.100-200cm.tif', 'FC.0-5cm.tif', 'ksat_5-15cm.tif', 'Longitud', 'AWC_100-200cm.tif', 'ksat_60-100cm.tif', 'Latitud', 'n_30-60cm.tif', 'valor_humedad_suelo1', 'Total_AWC_0_200cm.tif', 'PIRange_Bulkd.100-200cm.tif', 'Tex_Class.0-5cm.tif', 'Clay.100-200cm.tif', 'AvMoist.60-100cm.tif', 'AWC_0-5cm.tif', 'PP', 'n_60-100cm.tif', 'AvMoist.15-30cm.tif', 'ksat_15-30cm.tif', 'PIRange_Sand.30-60cm.tif', 'ksat_30-60cm.tif', 'FC.60-100cm.tif', 'Unnamed: 0', 'PWP.30-60cm.tif', 'n_15-30cm.tif', 'PIRange_Clay.5-15cm.tif']\n"
     ]
    }
   ],
   "source": [
    "columns_data_0 = set(data_0.columns)\n",
    "columns_data_1 = set(data_1.columns)\n",
    "\n",
    "# Columnas en común\n",
    "common_columns = columns_data_0 & columns_data_1\n",
    "\n",
    "print(\"Columnas en común:\", list(common_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1_aux = data_1[list(common_columns)]\n",
    "data_0_aux = data_0[list(common_columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Región', 'Sistema Georeferencia', 'Cota (m.s.n.m)', 'Latitud', 'Longitud', 'valor_humedad_suelo2', 'Unnamed: 0',\n",
    "        'valor_humedad_suelo3', 'valor_humedad_suelo4', 'Tipo Remoción en masa', 'Factor desencadenante', 'Comuna', 'Fecha Evento']\n",
    "data_1_aux = data_1_aux.drop(columns=cols)\n",
    "data_0_aux = data_0_aux.drop(columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((531, 107), (458, 107))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_0_aux.shape, data_1_aux.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_df1 = set(data_0_aux.columns)\n",
    "columns_df2 = set(data_1_aux.columns)\n",
    "\n",
    "columns_different = columns_df1.symmetric_difference(columns_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicated_columns = data_0_aux.columns[data_0_aux.columns.duplicated()]\n",
    "duplicated_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data_0_aux, data_1_aux])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()\n",
    "data_y = data.Valor\n",
    "data = data.drop(columns='Valor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" checked><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, data_y, random_state=42) \n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SelectFromModel(estimator=LogisticRegression(C=0.5, penalty=&#x27;l1&#x27;,\n",
       "                                             random_state=42,\n",
       "                                             solver=&#x27;liblinear&#x27;))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SelectFromModel</label><div class=\"sk-toggleable__content\"><pre>SelectFromModel(estimator=LogisticRegression(C=0.5, penalty=&#x27;l1&#x27;,\n",
       "                                             random_state=42,\n",
       "                                             solver=&#x27;liblinear&#x27;))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.5, penalty=&#x27;l1&#x27;, random_state=42, solver=&#x27;liblinear&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.5, penalty=&#x27;l1&#x27;, random_state=42, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "SelectFromModel(estimator=LogisticRegression(C=0.5, penalty='l1',\n",
       "                                             random_state=42,\n",
       "                                             solver='liblinear'))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "sel_ = SelectFromModel(LogisticRegression(C=0.5, penalty='l1', solver='liblinear', random_state=42))\n",
    "sel_.fit(scaler.transform(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['x4', 'x8', 'x10', 'x12', 'x16', 'x33', 'x36', 'x38', 'x43', 'x47',\n",
       "       'x48', 'x49', 'x56', 'x58', 'x70', 'x73', 'x76', 'x85', 'x89',\n",
       "       'x91', 'x92', 'x100', 'x101', 'x105'], dtype=object)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel_.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = data.columns[[2,4,13,20,25,32,33,35,36,39,42,46,60,74,75,85,86,91,92,94,95,96,101,102,103]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = X_train[cols]\n",
    "#X_test = X_test[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_selected = sel_.transform(scaler.transform(X_train))\n",
    "X_test_selected = sel_.transform(scaler.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de entrenamiendo: (408, 106), Numero de test: (136, 106)\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Precisión de 79.41176470588235\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2, 0.3],\n",
    "    'reg_alpha': [0, 0.1, 0.5, 1.0],\n",
    "    'reg_lambda': [0, 1.0, 10.0]\n",
    "}\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(data, data_y, test_size=0.2)\n",
    "print(f'Numero de entrenamiendo: {X_train.shape}, Numero de test: {X_test.shape}')\n",
    "model = XGBClassifier()\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_grid, n_iter=50, scoring='accuracy', cv=5, verbose=1, random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "acc = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "print(f'Precisión de {acc*100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0_aux = data_0_aux.drop(columns='Valor')\n",
    "data_1_aux = data_1_aux.drop(columns='Valor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted probabilities: [0.23333333 0.23333333 0.5        0.1        0.35       0.33333333\n",
      " 0.         0.         0.43333333 0.         0.7892096  0.7110175\n",
      " 0.7110175  0.64667713 0.43492063 0.15714286 0.06666667 0.13333333\n",
      " 0.13333333 0.87651515 1.         0.49117647 0.72178788 0.58387302\n",
      " 0.56783568 0.3        0.64667713 0.08       0.7110175  0.7110175\n",
      " 0.2        0.23714286 0.30714286 0.30714286 0.         0.28\n",
      " 0.2        0.1        0.10967742 0.10967742 0.67651515 0.4\n",
      " 0.58888889 0.34603175 0.15       0.15       0.68435829 0.48435829\n",
      " 0.84114286 0.72666667 0.8        0.6        0.6        0.10967742\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.10967742 0.         0.31666667 0.46666667 0.53333333\n",
      " 0.26967742 0.1        0.1        0.50967742 0.3        0.66666667\n",
      " 0.23333333 0.88261039 0.93942857 0.48       0.48       0.08\n",
      " 0.08       0.         0.33333333 0.33333333 0.33333333 0.93333333\n",
      " 0.4        0.48333333 0.4        0.4        0.73333333 0.51111111\n",
      " 0.82222222 0.87651515 0.35       0.1        0.1        0.15\n",
      " 0.20967742 0.65714286 0.13333333 0.13333333 0.35       0.35\n",
      " 0.5        0.         0.54603175 0.73333333 0.45967742 0.2\n",
      " 0.58888889 0.54603175 0.6        0.30967742 0.38888889 0.10967742\n",
      " 0.30967742 0.13333333 0.23333333 0.6        0.7        0.45967742\n",
      " 0.15714286 0.         0.         0.         0.         0.38888889\n",
      " 0.         0.         0.18888889 0.18888889 0.18888889 0.15714286\n",
      " 0.2        0.46682028 0.         0.         0.44603175 0.\n",
      " 0.184      0.2        0.5        0.69047619 0.         0.2\n",
      " 0.2        0.2        0.35       0.54301075 0.16666667 0.\n",
      " 0.56666667 0.         0.         0.         0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.34856631 0.         0.49856631 0.2        0.06666667\n",
      " 0.4        0.         0.46666667 0.6        0.55570917 0.\n",
      " 0.         0.6        0.         0.55714286 0.06666667 0.50967742\n",
      " 0.2        0.20784314 0.         0.         0.         0.85714286\n",
      " 0.56666667 0.75       0.83714286 0.         0.63       0.45\n",
      " 0.57059621 0.65       0.4        0.93942857 0.2        0.2\n",
      " 0.         0.30714286 0.30714286 0.30714286 0.         0.30714286\n",
      " 0.30714286 0.         0.         0.         0.         0.23714286\n",
      " 0.23714286 0.43333333 0.43333333 0.2        0.2        0.28\n",
      " 0.28       0.08       0.         0.06666667 0.5610175  0.64667713\n",
      " 0.5610175  0.5610175  0.5610175  0.5610175  0.5610175  0.5610175\n",
      " 0.5610175  0.5610175  0.76666667 0.5        0.71832468 0.71832468\n",
      " 0.71118182 0.7892096  0.868      0.85914424 0.85914424 0.72178788\n",
      " 0.5892096  0.72178788 0.84667713 0.51118182 0.73435829 0.7110175\n",
      " 0.84667713 0.7110175  0.76783568 0.7110175  0.7110175  0.71264352\n",
      " 0.58277339 0.64667713 0.74121494 0.74667713 0.69121494 0.\n",
      " 0.76783568 0.64667713 0.7110175  0.56783568 0.7110175  0.7110175\n",
      " 0.56783568 0.56783568 0.7110175  0.84667713 0.76783568 0.64667713\n",
      " 0.84667713 0.64667713 0.         0.7110175  0.7110175  0.73435829\n",
      " 0.6288961  0.73435829 0.6        0.78435829 0.7110175  0.7110175\n",
      " 0.84667713 0.74667713 0.71610672 0.         0.         0.\n",
      " 0.         0.         0.2        0.1        0.3        0.2\n",
      " 0.78461538 0.66666667 0.3        0.         0.         0.15714286\n",
      " 0.05       0.41733333 0.4        0.5        0.67936508 0.61428571\n",
      " 0.26666667 0.42777778 0.64444444 0.33333333 0.74603175 0.44444444\n",
      " 0.65       0.44444444 0.13333333 0.24444444 0.53333333 0.33333333\n",
      " 0.24444444 0.2        0.24444444 0.24444444 0.44444444 0.44444444\n",
      " 0.4        0.4        0.2        0.4        0.76       0.5610175\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.5        0.75714286 0.5        0.63499134\n",
      " 0.46666667 0.46666667 0.84175824 0.84175824 0.64444444 0.93333333\n",
      " 0.55714286 0.55       0.26666667 0.58387302 0.58461538 0.74927706\n",
      " 0.46682028 0.46682028 0.3        0.3        0.73492063 0.14318182\n",
      " 0.65714286 0.65714286 0.55714286 0.5        0.35714286 0.36239316\n",
      " 0.4        0.484      0.88435829 0.86666667 0.         0.74121494\n",
      " 0.72178788 0.64667713 0.06666667 0.55714286 0.4        0.55714286\n",
      " 0.         0.15       0.         0.         0.50967742 0.49856631\n",
      " 0.4        0.63333333 0.4        0.06666667 0.66666667 0.\n",
      " 0.49117647 0.5        0.08       0.16       0.31733333 0.2\n",
      " 0.2        0.         0.         0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.31733333 0.31733333 0.31733333 0.33333333 0.\n",
      " 0.2        0.         0.         0.36428571 0.41666667 0.\n",
      " 0.10967742 0.10967742 0.10967742 0.         0.10967742 0.26666667\n",
      " 0.26666667 0.26666667 0.06666667 0.53333333 0.53333333 0.53333333\n",
      " 0.         0.         0.78845455 0.50967742 0.3        0.2\n",
      " 0.2        0.3        0.4        0.         0.25       0.40967742\n",
      " 0.2        0.40967742 0.25       0.2        0.10967742 0.2\n",
      " 0.61587302 0.46666667 0.51666667 0.46666667 0.6        0.664\n",
      " 0.2        0.2        0.         0.         0.06666667 0.06666667\n",
      " 0.06666667 0.         0.         0.58095238 0.2        0.35\n",
      " 0.10967742 0.         0.10967742 0.71428571 0.3        0.43492063\n",
      " 0.06666667 0.24444444 0.5        0.2        0.         0.53333333\n",
      " 0.4        0.76       0.7        0.1        0.43333333 0.43333333\n",
      " 0.28       0.55847619 0.49180952 0.71118182 0.89180952 0.75714286\n",
      " 0.55847619 0.968      0.968      0.918      0.918      0.2\n",
      " 0.2        0.2        0.2        0.         0.         0.31733333\n",
      " 0.31733333 0.30967742 0.30967742 0.30967742 0.30967742 0.7\n",
      " 0.7        0.7        0.7        0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.544\n",
      " 0.         0.544      0.         0.13333333 0.28       0.1\n",
      " 0.13333333 0.28       0.1       ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "class PUBagging:\n",
    "    def __init__(self, num_iterations=10, sample_ratio=1.0, random_state=42):\n",
    "        self.num_iterations = num_iterations\n",
    "        self.sample_ratio = sample_ratio\n",
    "        self.random_state = random_state\n",
    "        self.classifiers = []\n",
    "        self.probabilities = None\n",
    "\n",
    "    def fit(self, landslide_samples, unlabeled_samples):\n",
    "        np.random.seed(self.random_state)\n",
    "\n",
    "        if isinstance(landslide_samples, np.ndarray):\n",
    "            landslide_samples_array = landslide_samples\n",
    "        else:\n",
    "            landslide_samples_array = landslide_samples.values  # Convert to numpy array if it's a DataFrame\n",
    "\n",
    "        if isinstance(unlabeled_samples, np.ndarray):\n",
    "            unlabeled_samples_array = unlabeled_samples\n",
    "        else:\n",
    "            unlabeled_samples_array = unlabeled_samples.values  # Convert to numpy array if it's a DataFrame\n",
    "\n",
    "        num_samples = len(landslide_samples_array)\n",
    "        num_unlabeled = len(unlabeled_samples_array)\n",
    "\n",
    "        for _ in range(self.num_iterations):\n",
    "            # Step 1: Sample equal number of unlabeled samples as non-landslide samples\n",
    "            non_landslide_indices = np.random.choice(num_unlabeled, size=int(num_samples * self.sample_ratio), replace=False)\n",
    "            non_landslide_samples = unlabeled_samples_array[non_landslide_indices]\n",
    "\n",
    "            # Combine with landslide samples to form training set\n",
    "            X_train = np.vstack((landslide_samples_array, non_landslide_samples))\n",
    "            y_train = np.hstack((np.ones(num_samples), np.zeros(len(non_landslide_samples))))\n",
    "\n",
    "            # Step 2: Train decision tree classifier\n",
    "            clf = DecisionTreeClassifier(random_state=self.random_state)\n",
    "            clf.fit(X_train, y_train)\n",
    "            self.classifiers.append(clf)\n",
    "\n",
    "            # Step 3: Predict probability of being landslide for unlabeled samples\n",
    "            prob_landslide = clf.predict_proba(unlabeled_samples_array)[:, 1]\n",
    "\n",
    "            if self.probabilities is None:\n",
    "                self.probabilities = prob_landslide\n",
    "            else:\n",
    "                self.probabilities += prob_landslide\n",
    "\n",
    "        # Step 4: Average probabilities over iterations\n",
    "        self.probabilities /= self.num_iterations\n",
    "\n",
    "    def predict_proba(self, unlabeled_samples):\n",
    "        if isinstance(unlabeled_samples, np.ndarray):\n",
    "            return self.probabilities\n",
    "        else:\n",
    "            return self.probabilities[:len(unlabeled_samples)]  # Return probabilities for original DataFrame\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == '__main__':\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Example data (replace with your own data)\n",
    "    landslide_samples = data_1_aux# Example landslide samples as DataFrame\n",
    "    unlabeled_samples = data_0_aux# Example unlabeled samples as DataFrame\n",
    "\n",
    "    # Create PU Bagging instance\n",
    "    pu_bagging = PUBagging(num_iterations=5, sample_ratio=0.4, random_state=42)\n",
    "\n",
    "    # Fit the model\n",
    "    pu_bagging.fit(landslide_samples, unlabeled_samples)\n",
    "\n",
    "    # Predict probabilities for unlabeled samples\n",
    "    probabilities = pu_bagging.predict_proba(unlabeled_samples)\n",
    "    print(\"Predicted probabilities:\", probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muestras no deslizamiento seleccionadas:\n",
      "     theta_r_5-15cm.tif  Bulkd.0-5cm.tif  Bulkd.60-100cm.tif  \\\n",
      "0                 0.075            0.798               0.919   \n",
      "1                 0.075            0.798               0.919   \n",
      "3                 0.117            0.646               0.782   \n",
      "4                 0.118            0.617               0.826   \n",
      "5                 0.077            1.049               0.667   \n",
      "..                  ...              ...                 ...   \n",
      "526               0.101            1.146               0.913   \n",
      "527               0.108            1.381               1.177   \n",
      "528               0.105            1.122               0.945   \n",
      "529               0.101            1.146               0.913   \n",
      "530               0.108            1.381               1.177   \n",
      "\n",
      "     Clay.15-30cm.tif  PIRange_Sand.100-200cm.tif  PIRange_Bulkd.0-5cm.tif  \\\n",
      "0            7.984000                   47.767002                    0.602   \n",
      "1            7.984000                   47.767002                    0.602   \n",
      "3           27.987000                   47.086002                    0.593   \n",
      "4           35.040001                   47.029003                    0.591   \n",
      "5           12.498000                   47.201000                    0.639   \n",
      "..                ...                         ...                      ...   \n",
      "526         25.270000                   46.548996                    0.595   \n",
      "527         16.777000                   47.888000                    0.588   \n",
      "528         26.052000                   46.601997                    0.599   \n",
      "529         25.270000                   46.548996                    0.595   \n",
      "530         16.777000                   47.888000                    0.588   \n",
      "\n",
      "     ksat_100-200cm.tif  theta_s_60-100cm.tif  Sand.100-200cm.tif  \\\n",
      "0            113.862000                 0.494           58.348000   \n",
      "1            113.862000                 0.494           58.348000   \n",
      "3            322.029999                 0.592           44.424999   \n",
      "4            204.011993                 0.608           59.883999   \n",
      "5            259.528992                 0.594           73.137001   \n",
      "..                  ...                   ...                 ...   \n",
      "526          106.301003                 0.562           43.063000   \n",
      "527           44.862999                 0.459           44.988998   \n",
      "528           94.513000                 0.555           42.269001   \n",
      "529          106.301003                 0.562           43.063000   \n",
      "530           44.862999                 0.459           44.988998   \n",
      "\n",
      "     Silt.30-60cm.tif  ...          PP  n_60-100cm.tif  AvMoist.15-30cm.tif  \\\n",
      "0           38.666000  ...   11.358299           1.515                0.266   \n",
      "1           38.666000  ...  282.989359           1.515                0.266   \n",
      "3           26.843000  ...  101.366892           1.361                0.259   \n",
      "4            4.602000  ...    0.000000           1.296                0.224   \n",
      "5           18.895000  ...    8.399357           1.404                0.173   \n",
      "..                ...  ...         ...             ...                  ...   \n",
      "526         25.250000  ...    3.830269           1.356                0.190   \n",
      "527         35.445999  ...    6.876428           1.431                0.158   \n",
      "528         25.201000  ...  327.677736           1.354                0.191   \n",
      "529         25.250000  ...  327.677736           1.356                0.190   \n",
      "530         35.445999  ...  329.135137           1.431                0.158   \n",
      "\n",
      "     ksat_15-30cm.tif  PIRange_Sand.30-60cm.tif  ksat_30-60cm.tif  \\\n",
      "0          343.113007                 34.486000        294.175995   \n",
      "1          343.113007                 34.486000        294.175995   \n",
      "3          319.493988                 33.466999        271.362000   \n",
      "4          173.709000                 33.394001        153.057007   \n",
      "5          177.292007                 33.823002        157.455994   \n",
      "..                ...                       ...               ...   \n",
      "526         59.228001                 33.021000         39.492001   \n",
      "527         23.757999                 34.564999         18.684999   \n",
      "528         57.912998                 33.084999         46.259998   \n",
      "529         59.228001                 33.021000         39.492001   \n",
      "530         23.757999                 34.564999         18.684999   \n",
      "\n",
      "     FC.60-100cm.tif  PWP.30-60cm.tif  n_15-30cm.tif  PIRange_Clay.5-15cm.tif  \n",
      "0              0.324            0.120          1.526                30.387001  \n",
      "1              0.324            0.120          1.526                30.387001  \n",
      "3              0.433            0.212          1.360                23.247002  \n",
      "4              0.423            0.244          1.291                24.039999  \n",
      "5              0.372            0.109          1.468                22.858002  \n",
      "..               ...              ...            ...                      ...  \n",
      "526            0.407            0.170          1.395                23.351999  \n",
      "527            0.325            0.130          1.432                25.614998  \n",
      "528            0.403            0.175          1.392                24.103001  \n",
      "529            0.407            0.170          1.395                23.351999  \n",
      "530            0.325            0.130          1.432                25.614998  \n",
      "\n",
      "[341 rows x 106 columns]\n"
     ]
    }
   ],
   "source": [
    "# Suponiendo que ya tienes las probabilidades predichas por pu_bagging.predict_proba(unlabeled_samples)\n",
    "\n",
    "# Umbral para seleccionar muestras no deslizamiento\n",
    "threshold = 0.5  # Puedes ajustar este umbral según tus necesidades\n",
    "\n",
    "# Filtrar muestras no deslizamiento por debajo del umbral\n",
    "non_landslide_indices = np.where(probabilities < threshold)[0]\n",
    "selected_non_landslide_samples = unlabeled_samples.iloc[non_landslide_indices]\n",
    "\n",
    "# Ejemplo de cómo podrías utilizar las muestras seleccionadas\n",
    "print(\"Muestras no deslizamiento seleccionadas:\")\n",
    "print(selected_non_landslide_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ivo\\AppData\\Local\\Temp\\ipykernel_7092\\3564039078.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  selected_non_landslide_samples['Valor'] = 0\n"
     ]
    }
   ],
   "source": [
    "selected_non_landslide_samples['Valor'] = 0\n",
    "data_1_aux['Valor'] = 1\n",
    "\n",
    "data = pd.concat([data_1_aux, selected_non_landslide_samples])\n",
    "data_y = data['Valor']\n",
    "data = data.drop(columns=['Valor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((458, 107), (341, 107))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1_aux.shape, selected_non_landslide_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de entrenamiendo: (639, 106), Numero de test: (160, 106)\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Precisión de 89.375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2, 0.3],\n",
    "    'reg_alpha': [0, 0.1, 0.5, 1.0],\n",
    "    'reg_lambda': [0, 1.0, 10.0]\n",
    "}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, data_y, test_size=0.2)\n",
    "print(f'Numero de entrenamiendo: {X_train.shape}, Numero de test: {X_test.shape}')\n",
    "model = XGBClassifier()\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_grid, n_iter=50, scoring='accuracy', cv=5, verbose=1, random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "acc = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "print(f'Precisión de {acc*100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geotiff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
